<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dual-Camera LiDAR Fusion | Xingnan Zhou</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif; line-height: 1.6; color: #333; background-color: #f5f6f8; }

        .header { background: linear-gradient(135deg, #0f1923 0%, #1a2940 50%, #243b55 100%); color: white; padding: 60px 20px 50px; text-align: center; position: relative; overflow: hidden; }
        .header::before { content: ''; position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: radial-gradient(ellipse at 30% 50%, rgba(78, 205, 196, 0.08) 0%, transparent 60%), radial-gradient(ellipse at 70% 50%, rgba(69, 183, 209, 0.06) 0%, transparent 60%); pointer-events: none; }
        .header h1 { font-size: 1.9em; font-weight: 700; margin-bottom: 12px; position: relative; max-width: 800px; margin-left: auto; margin-right: auto; line-height: 1.3; }
        .header .authors { font-size: 1.05em; opacity: 0.9; margin-top: 10px; position: relative; }
        .header .affiliation { font-size: 0.95em; opacity: 0.7; margin-top: 6px; position: relative; }
        .header .venue { display: inline-block; margin-top: 14px; padding: 6px 20px; background: rgba(232, 168, 56, 0.3); border: 1px solid rgba(232, 168, 56, 0.6); border-radius: 20px; font-size: 0.9em; position: relative; }
        .header-links { margin-top: 22px; display: flex; justify-content: center; gap: 12px; flex-wrap: wrap; position: relative; }
        .header-links a { display: inline-flex; align-items: center; gap: 6px; color: white; text-decoration: none; padding: 8px 18px; border: 1.5px solid rgba(255,255,255,0.35); border-radius: 6px; font-size: 0.9em; transition: all 0.2s ease; }
        .header-links a:hover { background: rgba(255,255,255,0.12); border-color: rgba(255,255,255,0.6); }

        .container { max-width: 920px; margin: 0 auto; padding: 36px 24px; }
        section { background: white; border-radius: 10px; padding: 32px 36px; margin-bottom: 24px; box-shadow: 0 1px 4px rgba(0,0,0,0.06); }
        h2 { font-size: 1.5em; color: #1a2940; margin-bottom: 18px; padding-bottom: 8px; border-bottom: 3px solid #4ECDC4; }
        h3 { font-size: 1.15em; color: #1a2940; margin: 20px 0 10px; }
        p { margin-bottom: 12px; line-height: 1.75; }

        .abstract-box { background: #f8f9fb; border-left: 4px solid #4ECDC4; padding: 20px 24px; border-radius: 0 8px 8px 0; margin: 16px 0; font-size: 0.95em; line-height: 1.8; }
        .fig-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0; }
        .fig-card { background: #f8f9fb; border-radius: 8px; padding: 16px; text-align: center; }
        .fig-card img { max-width: 100%; border-radius: 6px; margin-bottom: 8px; }
        .fig-card .caption { font-size: 0.85em; color: #666; line-height: 1.4; }
        .fig-full { background: #f8f9fb; border-radius: 8px; padding: 16px; text-align: center; margin: 20px 0; }
        .fig-full img { max-width: 100%; border-radius: 6px; margin-bottom: 8px; }
        .fig-full .caption { font-size: 0.85em; color: #666; line-height: 1.4; }

        table { width: 100%; border-collapse: collapse; margin: 16px 0; font-size: 0.92em; }
        th { background: #1a2940; color: white; padding: 10px 14px; text-align: center; font-weight: 600; }
        td { padding: 9px 14px; text-align: center; border-bottom: 1px solid #e8e8e8; }
        tr:nth-child(even) td { background: #f8f9fb; }
        tr:hover td { background: #e8f7f6; }
        .highlight { font-weight: 700; color: #1a7a72; }

        .stats-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(160px, 1fr)); gap: 14px; margin: 20px 0; }
        .stat-card { text-align: center; padding: 18px; background: #f8f9fb; border-radius: 8px; }
        .stat-card .number { font-size: 1.7em; font-weight: 700; color: #1a2940; }
        .stat-card .label { font-size: 0.82em; color: #888; margin-top: 2px; }

        .method-list { list-style: none; padding: 0; }
        .method-list li { padding: 10px 0; padding-left: 28px; position: relative; border-bottom: 1px solid #f0f0f0; }
        .method-list li::before { content: ''; position: absolute; left: 0; top: 16px; width: 12px; height: 12px; background: #4ECDC4; border-radius: 50%; }
        .method-list li:last-child { border-bottom: none; }

        .footer { background-color: #0f1923; color: white; text-align: center; padding: 28px 20px; margin-top: 16px; }
        .footer p { opacity: 0.7; text-align: center; font-size: 0.9em; }
        .footer a { color: #4ECDC4; text-decoration: none; }

        @media (max-width: 768px) { .header h1 { font-size: 1.4em; } section { padding: 24px 20px; } .fig-grid { grid-template-columns: 1fr; } }
    </style>
</head>
<body>

    <div class="header">
        <h1>Dual-Camera LiDAR Fusion for Occlusion-Robust 3D Detection in Urban Driving Simulation</h1>
        <div class="authors"><strong>Xingnan Zhou</strong> and Ciprian Alecsandru</div>
        <div class="affiliation">Concordia University, Montreal, QC, Canada</div>
        <div class="venue">In Preparation &mdash; Submitted to MDPI Sustainability, 2026</div>
        <div class="header-links">
            <a href="assets/bev-lidar-fusion-paper.pdf">Paper PDF</a>
            <a href="#results">Results</a>
            <a href="#method">Method</a>
            <a href="index.html">Back to Portfolio</a>
        </div>
    </div>

    <div class="container">

        <section id="abstract">
            <h2>Abstract</h2>
            <div class="abstract-box">
                Three-dimensional object detection from LiDAR point clouds is a cornerstone of autonomous driving perception,
                yet single-sensor systems remain vulnerable to occlusion in complex urban environments.
                This paper proposes an <strong>asymmetric dual-camera LiDAR fusion framework</strong> that combines
                a PointPillar-based 3D LiDAR detector with YOLOv8-based 2D detections from two complementary
                camera viewpoints: a <strong>drone</strong> (top-down, 40m altitude) and a subject-vehicle
                <strong>forward camera</strong>. The fusion operates at the decision level (late fusion),
                where camera-confirmed LiDAR detections receive confidence boosts while unconfirmed
                low-confidence detections are suppressed. The full dual-camera fusion achieves
                <strong>+2.88 pp mAP@0.5 (+11.2% relative; sign test p = 0.031)</strong>,
                with all five seeds showing positive improvement.
            </div>
            <div class="stats-grid">
                <div class="stat-card">
                    <div class="number">+11.2%</div>
                    <div class="label">mAP@0.5 Improvement</div>
                </div>
                <div class="stat-card">
                    <div class="number">5/5</div>
                    <div class="label">Seeds Positive</div>
                </div>
                <div class="stat-card">
                    <div class="number">12,308</div>
                    <div class="label">Object Annotations</div>
                </div>
                <div class="stat-card">
                    <div class="number">650</div>
                    <div class="label">CARLA Frames</div>
                </div>
            </div>
        </section>

        <section id="method">
            <h2>Method</h2>

            <div class="fig-full">
                <img src="assets/bev_lidar/fig_pipeline.png" alt="Asymmetric dual-camera LiDAR fusion pipeline">
                <div class="caption">Overview of the proposed dual-camera LiDAR fusion pipeline. The PointPillar 3D detector
                    and two independent YOLOv8 2D detectors produce detections from their respective sensors.
                    The asymmetric late-fusion module refines confidence scores based on camera-specific boost and suppress rules.</div>
            </div>

            <h3>Three-Stage Pipeline</h3>
            <ul class="method-list">
                <li>
                    <strong>3D LiDAR Detection (PointPillars)</strong> &mdash; Processes ego vehicle&rsquo;s
                    64-channel LiDAR point cloud into 3D bounding boxes with class labels and confidence scores
                </li>
                <li>
                    <strong>2D Camera Detection (YOLOv8)</strong> &mdash; Two independent YOLOv8m models
                    process drone (top-down, 40m) and forward camera images, fine-tuned on CARLA-rendered data
                </li>
                <li>
                    <strong>Asymmetric Late Fusion</strong> &mdash; Camera-confirmed LiDAR detections receive
                    confidence boosts (&times;1.15 single-cam, &times;1.30 dual-cam), while
                    unconfirmed low-confidence detections in the drone&rsquo;s FOV are suppressed (&times;0.75)
                </li>
            </ul>

            <h3>Key Design: Asymmetric Treatment</h3>
            <p>
                The <strong>drone camera</strong> has near-complete overhead coverage, so it can both
                <em>boost</em> confirmed detections and <em>suppress</em> unconfirmed ones.
                The <strong>forward camera</strong> covers only a narrow frontal sector, so it applies
                <em>boost-only</em> to avoid penalizing legitimate detections outside its FOV.
                This asymmetry is grounded in each sensor&rsquo;s coverage characteristics.
            </p>
        </section>

        <section id="results">
            <h2>Results</h2>
            <p>
                Evaluated on a CARLA Town10HD dataset with 650 frames, 12,308 annotations (Car + Pedestrian),
                and five-seed repeated random sub-sampling validation.
            </p>

            <h3>Main Results</h3>
            <table>
                <thead>
                    <tr>
                        <th>Configuration</th>
                        <th>Car AP</th>
                        <th>Ped AP</th>
                        <th>mAP@0.5</th>
                        <th>&Delta;</th>
                        <th>Sign Test</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>LiDAR-only</td>
                        <td>49.11</td>
                        <td>2.26</td>
                        <td>25.69 &plusmn; 3.89</td>
                        <td>&mdash;</td>
                        <td>&mdash;</td>
                    </tr>
                    <tr>
                        <td>LiDAR + Drone</td>
                        <td>51.48</td>
                        <td>2.86</td>
                        <td>27.17 &plusmn; 4.53</td>
                        <td>+1.48</td>
                        <td>p = 0.031*</td>
                    </tr>
                    <tr>
                        <td>LiDAR + SDC</td>
                        <td>49.08</td>
                        <td>3.53</td>
                        <td>26.31 &plusmn; 4.05</td>
                        <td>+0.62</td>
                        <td>p = 0.031*</td>
                    </tr>
                    <tr>
                        <td><strong>LiDAR + Drone + SDC</strong></td>
                        <td class="highlight">52.66</td>
                        <td class="highlight">4.46</td>
                        <td class="highlight">28.56 &plusmn; 3.78</td>
                        <td class="highlight">+2.88</td>
                        <td>p = 0.031*</td>
                    </tr>
                </tbody>
            </table>

            <div class="fig-grid">
                <div class="fig-card">
                    <img src="assets/bev_lidar/fig1_mAP05_ablation.png" alt="Fusion ablation study">
                    <div class="caption">Five-seed averaged mAP@0.5 with standard deviation error bars</div>
                </div>
                <div class="fig-card">
                    <img src="assets/bev_lidar/fig4_improvement_heatmap.png" alt="Improvement heatmap">
                    <div class="caption">Improvement over LiDAR-only baseline across all metrics and configurations. Bold borders indicate statistical significance.</div>
                </div>
            </div>

            <h3>Per-Seed Consistency</h3>
            <div class="fig-full">
                <img src="assets/bev_lidar/fig3_per_seed_mAP05.png" alt="Per-seed consistency" style="max-width: 600px;">
                <div class="caption">Per-seed mAP@0.5 consistency. All fusion configurations consistently outperform
                    the LiDAR-only baseline across all five random seeds (5/5 positive, sign test p = 0.031).</div>
            </div>

            <h3>Camera Contribution Ablation</h3>
            <table>
                <thead>
                    <tr>
                        <th>Configuration</th>
                        <th>mAP@0.5</th>
                        <th>&Delta; (pp)</th>
                        <th>Relative</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>LiDAR-only</td><td>25.69</td><td>&mdash;</td><td>&mdash;</td></tr>
                    <tr><td>+ SDC camera (boost only)</td><td>26.31</td><td>+0.62</td><td>+2.4%</td></tr>
                    <tr><td>+ Drone camera (boost+suppress)</td><td>27.17</td><td>+1.48</td><td>+5.8%</td></tr>
                    <tr><td class="highlight">+ Both cameras (asymmetric)</td><td class="highlight">28.56</td><td class="highlight">+2.88</td><td class="highlight">+11.2%</td></tr>
                </tbody>
            </table>
            <p>
                The drone camera contributes approximately <strong>2.4&times;</strong> the mAP gain of the forward camera,
                consistent with the overhead perspective&rsquo;s superior coverage for resolving occlusions.
                The combined gain exceeds the sum of individual contributions, demonstrating complementary viewpoint diversity.
            </p>
        </section>

        <section id="qualitative">
            <h2>Qualitative Examples</h2>
            <div class="fig-full">
                <img src="assets/bev_lidar/panel_000028.jpg" alt="Qualitative fusion examples">
                <div class="caption">Representative scenario showing drone view (top-left), SDC/ego view (top-right),
                    LiDAR-only BEV detections (bottom-left), and fused detections (bottom-right).
                    Fusion recovers occluded vehicles and suppresses false positives.</div>
            </div>
        </section>

        <section id="citation">
            <h2>Citation</h2>
            <pre style="background: #f8f9fb; padding: 16px; border-radius: 8px; font-size: 0.85em; overflow-x: auto; line-height: 1.6;">@article{zhou2026bevfusion,
  title={Dual-Camera LiDAR Fusion for Occlusion-Robust 3D Detection in Urban Driving Simulation},
  author={Zhou, Xingnan and Alecsandru, Ciprian},
  journal={Sustainability},
  year={2026},
  publisher={MDPI}
}</pre>
        </section>

    </div>

    <div class="footer">
        <p><a href="index.html">Xingnan Zhou</a> &middot; Concordia University, Montreal &middot; <a href="https://github.com/Jynxzzz">GitHub</a></p>
        <p style="margin-top: 6px;">&copy; 2026</p>
    </div>

</body>
</html>
