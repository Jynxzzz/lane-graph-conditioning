<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Spatial Attention Visualization | Xingnan Zhou</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif; line-height: 1.6; color: #333; background-color: #f5f6f8; }

        .header { background: linear-gradient(135deg, #0f1923 0%, #1a2940 50%, #243b55 100%); color: white; padding: 60px 20px 50px; text-align: center; position: relative; overflow: hidden; }
        .header::before { content: ''; position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: radial-gradient(ellipse at 30% 50%, rgba(78, 205, 196, 0.08) 0%, transparent 60%), radial-gradient(ellipse at 70% 50%, rgba(69, 183, 209, 0.06) 0%, transparent 60%); pointer-events: none; }
        .header h1 { font-size: 1.7em; font-weight: 700; margin-bottom: 12px; position: relative; max-width: 820px; margin-left: auto; margin-right: auto; line-height: 1.3; }
        .header .authors { font-size: 1.05em; opacity: 0.9; margin-top: 10px; position: relative; }
        .header .affiliation { font-size: 0.95em; opacity: 0.7; margin-top: 6px; position: relative; }
        .header .venue { display: inline-block; margin-top: 14px; padding: 6px 20px; background: rgba(232, 168, 56, 0.3); border: 1px solid rgba(232, 168, 56, 0.6); border-radius: 20px; font-size: 0.9em; position: relative; }
        .header-links { margin-top: 22px; display: flex; justify-content: center; gap: 12px; flex-wrap: wrap; position: relative; }
        .header-links a { display: inline-flex; align-items: center; gap: 6px; color: white; text-decoration: none; padding: 8px 18px; border: 1.5px solid rgba(255,255,255,0.35); border-radius: 6px; font-size: 0.9em; transition: all 0.2s ease; }
        .header-links a:hover { background: rgba(255,255,255,0.12); border-color: rgba(255,255,255,0.6); }

        .container { max-width: 920px; margin: 0 auto; padding: 36px 24px; }
        section { background: white; border-radius: 10px; padding: 32px 36px; margin-bottom: 24px; box-shadow: 0 1px 4px rgba(0,0,0,0.06); }
        h2 { font-size: 1.5em; color: #1a2940; margin-bottom: 18px; padding-bottom: 8px; border-bottom: 3px solid #4ECDC4; }
        h3 { font-size: 1.15em; color: #1a2940; margin: 20px 0 10px; }
        p { margin-bottom: 12px; line-height: 1.75; }

        .abstract-box { background: #f8f9fb; border-left: 4px solid #4ECDC4; padding: 20px 24px; border-radius: 0 8px 8px 0; margin: 16px 0; font-size: 0.95em; line-height: 1.8; }
        .fig-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0; }
        .fig-card { background: #f8f9fb; border-radius: 8px; padding: 16px; text-align: center; }
        .fig-card img { max-width: 100%; border-radius: 6px; margin-bottom: 8px; }
        .fig-card .caption { font-size: 0.85em; color: #666; line-height: 1.4; }
        .fig-full { background: #f8f9fb; border-radius: 8px; padding: 16px; text-align: center; margin: 20px 0; }
        .fig-full img { max-width: 100%; border-radius: 6px; margin-bottom: 8px; }
        .fig-full .caption { font-size: 0.85em; color: #666; line-height: 1.4; }

        .stats-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(160px, 1fr)); gap: 14px; margin: 20px 0; }
        .stat-card { text-align: center; padding: 18px; background: #f8f9fb; border-radius: 8px; }
        .stat-card .number { font-size: 1.7em; font-weight: 700; color: #1a2940; }
        .stat-card .label { font-size: 0.82em; color: #888; margin-top: 2px; }

        .method-list { list-style: none; padding: 0; }
        .method-list li { padding: 10px 0; padding-left: 28px; position: relative; border-bottom: 1px solid #f0f0f0; }
        .method-list li::before { content: ''; position: absolute; left: 0; top: 16px; width: 12px; height: 12px; background: #4ECDC4; border-radius: 50%; }
        .method-list li:last-child { border-bottom: none; }

        .finding-card { background: #f8f9fb; border-left: 4px solid #e8a838; padding: 16px 20px; border-radius: 0 8px 8px 0; margin: 14px 0; }
        .finding-card strong { color: #1a2940; }

        .footer { background-color: #0f1923; color: white; text-align: center; padding: 28px 20px; margin-top: 16px; }
        .footer p { opacity: 0.7; text-align: center; font-size: 0.9em; }
        .footer a { color: #4ECDC4; text-decoration: none; }

        @media (max-width: 768px) { .header h1 { font-size: 1.3em; } section { padding: 24px 20px; } .fig-grid { grid-template-columns: 1fr; } }
    </style>
</head>
<body>

    <div class="header">
        <h1>Spatial Attention Visualization for Interpretable Trajectory Prediction in Autonomous Driving: Discovering Safety Blind Spots Through Counterfactual Analysis</h1>
        <div class="authors"><strong>Xingnan Zhou</strong> and Ciprian Alecsandru</div>
        <div class="affiliation">Concordia University, Montreal, QC, Canada</div>
        <div class="venue">In Preparation &mdash; Submitted to MDPI Sustainability, 2026</div>
        <div class="header-links">
            <a href="assets/attention-visualization-paper.pdf">Paper PDF</a>
            <a href="#findings">Key Findings</a>
            <a href="#method">Method</a>
            <a href="#visualizations">Visualizations</a>
            <a href="index.html">Back to Portfolio</a>
        </div>
    </div>

    <div class="container">

        <section id="abstract">
            <h2>Abstract</h2>
            <div class="abstract-box">
                While Transformer-based models have achieved state-of-the-art prediction performance,
                their internal attention mechanisms remain opaque. We present a <strong>spatial attention
                visualization framework</strong> that maps abstract Transformer attention weights onto
                bird&rsquo;s-eye-view (BEV) traffic scenes, providing the first spatially grounded
                interpretation of attention in trajectory prediction. Built upon <strong>MTR-Lite</strong>
                (8.48M parameters) trained on the Waymo Open Motion Dataset, our framework employs a novel
                <strong>spatial token bookkeeping</strong> mechanism. We discover that vulnerable road users
                (pedestrians and cyclists) receive up to <strong>60% less attention</strong> than vehicles
                at equivalent distances &mdash; a safety blind spot. We further introduce
                <strong>counterfactual attention analysis</strong> to isolate the causal effect of
                individual scene elements on model attention.
            </div>
            <div class="stats-grid">
                <div class="stat-card">
                    <div class="number">60%</div>
                    <div class="label">VRU Attention Deficit</div>
                </div>
                <div class="stat-card">
                    <div class="number">30%</div>
                    <div class="label">Pruning w/o Degradation</div>
                </div>
                <div class="stat-card">
                    <div class="number">8.48M</div>
                    <div class="label">MTR-Lite Parameters</div>
                </div>
                <div class="stat-card">
                    <div class="number">17.8K</div>
                    <div class="label">Waymo Scenes</div>
                </div>
            </div>
        </section>

        <section id="method">
            <h2>Method</h2>

            <h3>Framework Overview</h3>
            <p>
                Our visualization framework consists of three components built on top of an MTR-Lite
                Transformer trained on the Waymo Open Motion Dataset (20% subset, ~17,800 scenes):
            </p>
            <ul class="method-list">
                <li>
                    <strong>Attention-Capture Layers</strong> &mdash; Custom Transformer layers that extract
                    per-head attention weight matrices from every encoder and decoder layer without altering predictions
                </li>
                <li>
                    <strong>Spatial Token Bookkeeping</strong> &mdash; A bidirectional mapping between abstract
                    token indices and physical BEV coordinates, enabling projection of attention weights onto the traffic scene
                </li>
                <li>
                    <strong>Three Visualization Types</strong> &mdash; Space-attention BEV heatmaps (where),
                    time-attention refinement diagrams (how attention evolves across layers),
                    and lane-token activation maps (which road structures guide prediction)
                </li>
            </ul>

            <h3>MTR-Lite Architecture</h3>
            <p>
                A lightweight Motion Transformer variant with 4 encoder layers (global self-attention over
                32 agent + 64 map tokens), 4 decoder layers (agent cross-attention + map cross-attention),
                and 64 intention queries refined into K=6 output modes via NMS. Trained for 60 epochs
                on Waymo with AdamW, cosine annealing, and mixed-precision training.
            </p>

            <h3>Counterfactual Experiments</h3>
            <p>
                By directly editing scene dictionaries (removing agents, flipping traffic signals, injecting
                pedestrians at varying distances), we perform the first <strong>counterfactual attention
                analysis</strong> for trajectory prediction &mdash; enabling causal (not just correlational)
                claims about how scene elements influence model reasoning.
            </p>
        </section>

        <section id="visualizations">
            <h2>Attention Visualizations</h2>

            <div class="fig-grid">
                <div class="fig-card">
                    <img src="assets/attn_viz/scene_attention_matrix.png" alt="Scene encoder self-attention matrix">
                    <div class="caption">Scene encoder self-attention (Layer 4). The cyan boundary separates
                        32 agent tokens (left/top) from 64 map tokens (right/bottom). Agent-agent attention
                        (top-left quadrant) is notably stronger than agent-map attention.</div>
                </div>
                <div class="fig-card">
                    <img src="assets/attn_viz/decoder_map_attention.png" alt="Decoder map attention">
                    <div class="caption">Decoder map cross-attention (Layer 0). Each row is an intention query attending
                        to 64 lane tokens. Vertical striping reveals that certain lanes attract attention from
                        all queries &mdash; likely the ego agent&rsquo;s current and target lanes.</div>
                </div>
            </div>

            <div class="fig-full">
                <img src="assets/attn_viz/decoder_agent_attention_heads.png" alt="Decoder agent attention per head" style="max-width: 700px;">
                <div class="caption">Decoder agent cross-attention decomposed by attention head.
                    Different heads specialize in different aspects: some focus on the ego agent,
                    others attend broadly to nearby traffic participants.</div>
            </div>
        </section>

        <section id="findings">
            <h2>Key Findings</h2>

            <div class="finding-card">
                <strong>1. VRU Safety Blind Spot</strong>
                <p style="margin-top: 6px; margin-bottom: 0;">
                    Pedestrians and cyclists receive up to <strong>60% less attention</strong> than vehicles
                    at equivalent distances. This systematic under-attendance represents a safety-critical
                    blind spot in current Transformer architectures, with direct implications for collision risk
                    with vulnerable road users.
                </p>
            </div>

            <div class="finding-card">
                <strong>2. Progressive Attention Focusing</strong>
                <p style="margin-top: 6px; margin-bottom: 0;">
                    Entropy decreases from ~5.2 bits (Layer 0) to ~2.8 bits (Layer 3), confirming that
                    early layers perform broad scene surveying while late layers focus on task-relevant elements.
                    Late-layer sparsity enables <strong>~30% computational pruning</strong> with &lt;1% minADE@6 degradation.
                </p>
            </div>

            <div class="finding-card">
                <strong>3. Attention is Causally Reactive</strong>
                <p style="margin-top: 6px; margin-bottom: 0;">
                    Counterfactual experiments confirm that attention adapts when scene elements change &mdash;
                    it reflects genuine reasoning about current context, not memorized patterns.
                    When an agent is removed, freed attention flows preferentially to the next most relevant
                    element (target lane or next-closest agent), revealing a learned priority hierarchy.
                </p>
            </div>

            <div class="finding-card">
                <strong>4. Safety Certification Implications</strong>
                <p style="margin-top: 6px; margin-bottom: 0;">
                    The framework provides three types of evidence relevant to the EU AI Act and NHTSA
                    testing frameworks: spatial evidence (BEV overlays), causal evidence (counterfactual experiments),
                    and quantitative thresholds (e.g., minimum VRU attention threshold of 0.3 for collision avoidance).
                </p>
            </div>
        </section>

        <section id="citation">
            <h2>Citation</h2>
            <pre style="background: #f8f9fb; padding: 16px; border-radius: 8px; font-size: 0.85em; overflow-x: auto; line-height: 1.6;">@article{zhou2026attnviz,
  title={Spatial Attention Visualization for Interpretable Trajectory Prediction in Autonomous Driving: Discovering Safety Blind Spots Through Counterfactual Analysis},
  author={Zhou, Xingnan and Alecsandru, Ciprian},
  journal={Sustainability},
  year={2026},
  publisher={MDPI}
}</pre>
        </section>

    </div>

    <div class="footer">
        <p><a href="index.html">Xingnan Zhou</a> &middot; Concordia University, Montreal &middot; <a href="https://github.com/Jynxzzz">GitHub</a></p>
        <p style="margin-top: 6px;">&copy; 2026</p>
    </div>

</body>
</html>
